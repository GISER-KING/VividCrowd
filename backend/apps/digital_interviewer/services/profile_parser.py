"""
é¢è¯•å®˜ç”»åƒè§£ææœåŠ¡
è§£æPDF/Markdownæ ¼å¼çš„é¢è¯•å®˜ç”»åƒæ–‡ä»¶ï¼Œæå–å…³é”®ä¿¡æ¯
"""
import re
import json
from typing import Dict, Any, Optional
from pathlib import Path
import PyPDF2
import markdown


class InterviewerProfileParser:
    """é¢è¯•å®˜ç”»åƒè§£æå™¨"""

    def __init__(self):
        self.required_fields = ["name", "expertise_areas"]

    async def parse_file(self, file_path: str) -> Dict[str, Any]:
        """
        è§£æé¢è¯•å®˜ç”»åƒæ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            è§£æåçš„ç»“æ„åŒ–æ•°æ®
        """
        file_path_obj = Path(file_path)

        if not file_path_obj.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è§£ææ–¹æ³•
        if file_path_obj.suffix.lower() == '.pdf':
            raw_content = await self._parse_pdf(file_path)
        elif file_path_obj.suffix.lower() in ['.md', '.markdown']:
            raw_content = await self._parse_markdown(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path_obj.suffix}")

        # æå–ç»“æ„åŒ–ä¿¡æ¯
        profile_data = await self._extract_profile_data(raw_content)
        profile_data['raw_content'] = raw_content
        profile_data['source_file_path'] = file_path

        # éªŒè¯å¿…å¡«å­—æ®µ
        self._validate_profile(profile_data)

        return profile_data

    async def _parse_pdf(self, file_path: str) -> str:
        """è§£æPDFæ–‡ä»¶"""
        text = ""
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        return text.strip()

    async def _parse_markdown(self, file_path: str) -> str:
        """è§£æMarkdownæ–‡ä»¶"""
        from loguru import logger
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read().strip()
        logger.info(f"ğŸ“„ Markdownæ–‡ä»¶å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        logger.debug(f"ğŸ“„ æ–‡ä»¶å‰200å­—ç¬¦: {content[:200]}")
        return content

    async def _extract_profile_data(self, raw_content: str) -> Dict[str, Any]:
        """ä»åŸå§‹æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯"""
        from loguru import logger
        profile_data = {}

        logger.info("ğŸ” å¼€å§‹æå–ç»“æ„åŒ–ä¿¡æ¯...")

        # æå–å§“å - æ”¯æŒ **å§“å**: xxx æ ¼å¼
        name_pattern = r'\*{0,2}(?:å§“å|åå­—|Name)\*{0,2}[ï¼š:]\s*([^\n]+)'
        name_match = re.search(name_pattern, raw_content, re.IGNORECASE)
        if name_match:
            profile_data['name'] = name_match.group(1).strip()
            logger.info(f"âœ… æå–åˆ°å§“å: {profile_data['name']}")
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å§“åå­—æ®µï¼Œä½¿ç”¨çš„æ­£åˆ™: {name_pattern}")

        # æå–èŒä½
        title_pattern = r'\*{0,2}(?:èŒä½|Title)\*{0,2}[ï¼š:]\s*([^\n]+)'
        title_match = re.search(title_pattern, raw_content, re.IGNORECASE)
        if title_match:
            profile_data['title'] = title_match.group(1).strip()
            logger.info(f"âœ… æå–åˆ°èŒä½: {profile_data['title']}")
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°èŒä½å­—æ®µ")

        # æå–å…¬å¸
        company_pattern = r'\*{0,2}(?:å…¬å¸|Company)\*{0,2}[ï¼š:]\s*([^\n]+)'
        company_match = re.search(company_pattern, raw_content, re.IGNORECASE)
        if company_match:
            profile_data['company'] = company_match.group(1).strip()
            logger.info(f"âœ… æå–åˆ°å…¬å¸: {profile_data['company']}")
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å…¬å¸å­—æ®µ")

        # æå–ä¸“ä¸šé¢†åŸŸ
        expertise_pattern = r'\*{0,2}(?:ä¸“ä¸šé¢†åŸŸ|ä¸“é•¿|Expertise)\*{0,2}[ï¼š:]\s*([^\n]+)'
        expertise_match = re.search(expertise_pattern, raw_content, re.IGNORECASE)
        if expertise_match:
            profile_data['expertise_areas'] = expertise_match.group(1).strip()
            logger.info(f"âœ… æå–åˆ°ä¸“ä¸šé¢†åŸŸ: {profile_data['expertise_areas'][:50]}...")
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä¸“ä¸šé¢†åŸŸå­—æ®µï¼Œä½¿ç”¨çš„æ­£åˆ™: {expertise_pattern}")
            # æ‰“å°æ–‡ä»¶ä¸­åŒ…å«"ä¸“ä¸š"çš„è¡Œï¼Œå¸®åŠ©è°ƒè¯•
            lines_with_expertise = [line for line in raw_content.split('\n') if 'ä¸“ä¸š' in line or 'Expertise' in line.lower()]
            logger.debug(f"ğŸ“‹ æ–‡ä»¶ä¸­åŒ…å«'ä¸“ä¸š'çš„è¡Œ: {lines_with_expertise}")

        logger.info(f"ğŸ“Š æå–å®Œæˆï¼Œå…±æå–åˆ° {len(profile_data)} ä¸ªå­—æ®µ")
        return profile_data

    def _validate_profile(self, profile_data: Dict[str, Any]) -> None:
        """éªŒè¯ç”»åƒæ•°æ®çš„å®Œæ•´æ€§"""
        missing_fields = [field for field in self.required_fields if not profile_data.get(field)]
        if missing_fields:
            raise ValueError(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {', '.join(missing_fields)}")

    async def generate_system_prompt(self, profile_data: Dict[str, Any]) -> str:
        """æ ¹æ®ç”»åƒæ•°æ®ç”Ÿæˆç³»ç»Ÿæç¤ºè¯"""
        name = profile_data.get('name', 'é¢è¯•å®˜')
        title = profile_data.get('title', '')
        company = profile_data.get('company', '')
        expertise = profile_data.get('expertise_areas', '')

        prompt = f"""ä½ æ˜¯{name}"""
        if title:
            prompt += f"ï¼Œæ‹…ä»»{title}"
        if company:
            prompt += f"ï¼Œå°±èŒäº{company}"

        prompt += f"""ã€‚

ä½ çš„ä¸“ä¸šé¢†åŸŸï¼š{expertise}

ä½œä¸ºé¢è¯•å®˜ï¼Œä½ éœ€è¦ï¼š
1. æ ¹æ®å€™é€‰äººçš„å›ç­”æå‡ºæœ‰é’ˆå¯¹æ€§çš„é—®é¢˜
2. è¯„ä¼°å€™é€‰äººçš„ä¸“ä¸šèƒ½åŠ›ã€æ²Ÿé€šè¡¨è¾¾å’Œé—®é¢˜è§£å†³èƒ½åŠ›
3. ä¿æŒä¸“ä¸šã€å®¢è§‚ã€å‹å¥½çš„æ€åº¦
4. é€‚æ—¶è¿½é—®ä»¥æ·±å…¥äº†è§£å€™é€‰äººçš„èƒ½åŠ›

è¯·å§‹ç»ˆä¿æŒä½ çš„è§’è‰²è®¾å®šï¼Œç”¨ä¸“ä¸šçš„æ–¹å¼è¿›è¡Œé¢è¯•ã€‚"""

        return prompt
