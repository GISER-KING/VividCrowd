"""
æ•°å­—é¢è¯•å®˜åº”ç”¨ - FastAPIè·¯ç”±
"""
import os
import hashlib
import uuid
from typing import List, Optional
from fastapi import APIRouter, UploadFile, File, Form, HTTPException, WebSocket, WebSocketDisconnect, Depends
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from datetime import datetime

from backend.core.database import get_digital_interviewer_db
from backend.models.db_models import (
    InterviewerProfile,
    InterviewerProfileRegistry,
    InterviewSession,
    InterviewRound,
    InterviewEvaluation,
    InterviewKnowledge,
    InterviewExperienceSet
)
from backend.apps.digital_interviewer.services.profile_parser import InterviewerProfileParser
from backend.apps.digital_interviewer.services.experience_service import ExperienceService

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter(prefix="/digital-interviewer", tags=["æ•°å­—é¢è¯•å®˜"])

# æ–‡ä»¶ä¸Šä¼ ç›®å½•
UPLOAD_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "data", "interviewer_uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)

VIDEO_DIR = os.path.join(UPLOAD_DIR, "videos")
os.makedirs(VIDEO_DIR, exist_ok=True)

PROFILE_DIR = os.path.join(UPLOAD_DIR, "profiles")
os.makedirs(PROFILE_DIR, exist_ok=True)

# é¢ç»æ–‡ä»¶ä¸Šä¼ ç›®å½•
EXPERIENCE_DIR = os.path.join(UPLOAD_DIR, "experiences")
os.makedirs(EXPERIENCE_DIR, exist_ok=True)

# ç®€å†æ–‡ä»¶ä¸Šä¼ ç›®å½•
RESUME_DIR = os.path.join(UPLOAD_DIR, "resumes")
os.makedirs(RESUME_DIR, exist_ok=True)

# è™šæ‹Ÿäººå½¢è±¡ç›®å½•ï¼ˆç‹¬ç«‹çš„è™šæ‹Ÿäººå½¢è±¡åº“ï¼‰
DIGITAL_HUMANS_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "data", "digital_humans")
os.makedirs(DIGITAL_HUMANS_DIR, exist_ok=True)


def scan_all_digital_humans():
    """
    å¯åŠ¨æ—¶æ‰«ææ‰€æœ‰è™šæ‹Ÿäººå½¢è±¡
    """
    from loguru import logger
    from backend.core.database import get_digital_interviewer_db
    from backend.models.db_models import DigitalHuman

    logger.info("ğŸ­ å¼€å§‹æ‰«æè™šæ‹Ÿäººå½¢è±¡åº“...")

    db = next(get_digital_interviewer_db())
    try:
        if not os.path.exists(DIGITAL_HUMANS_DIR):
            logger.warning(f"âš ï¸ è™šæ‹Ÿäººå½¢è±¡ç›®å½•ä¸å­˜åœ¨: {DIGITAL_HUMANS_DIR}")
            return 0

        total_count = 0

        # éå†æ‰€æœ‰å­ç›®å½•
        for human_name in os.listdir(DIGITAL_HUMANS_DIR):
            human_path = os.path.join(DIGITAL_HUMANS_DIR, human_name)

            # åªå¤„ç†ç›®å½•
            if not os.path.isdir(human_path):
                continue

            logger.info(f"ğŸ­ æ‰«æè™šæ‹Ÿäººå½¢è±¡: {human_name}")

            # æ£€æŸ¥4ä¸ªå¿…éœ€çš„è§†é¢‘æ–‡ä»¶
            video_files = {
                'video_idle': 'idle.mp4',
                'video_speaking': 'speaking.mp4',
                'video_listening': 'listening.mp4',
                'video_thinking': 'thinking.mp4'
            }

            videos = {}
            all_found = True
            for field_name, filename in video_files.items():
                file_path = os.path.join(human_path, filename)
                if os.path.exists(file_path):
                    # è¿”å›ç›¸å¯¹è·¯å¾„ï¼Œä¾¿äºå‰ç«¯è®¿é—®
                    relative_path = f"/data/digital_humans/{human_name}/{filename}"
                    videos[field_name] = relative_path
                    logger.debug(f"  âœ… {filename}")
                else:
                    logger.warning(f"  âš ï¸ ç¼ºå°‘ {filename}")
                    all_found = False

            if not all_found:
                logger.warning(f"  âš ï¸ è™šæ‹Ÿäººå½¢è±¡ {human_name} è§†é¢‘ä¸å®Œæ•´ï¼Œè·³è¿‡")
                continue

            # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨è¯¥å½¢è±¡
            existing = db.query(DigitalHuman).filter_by(name=human_name).first()

            if existing:
                # æ›´æ–°ç°æœ‰è®°å½•
                for key, value in videos.items():
                    setattr(existing, key, value)
                existing.is_active = True
                logger.info(f"  ğŸ”„ æ›´æ–°è™šæ‹Ÿäººå½¢è±¡: {human_name}")
            else:
                # åˆ›å»ºæ–°è®°å½•
                # è‡ªåŠ¨ç”Ÿæˆæ˜¾ç¤ºåç§°ï¼ˆå°†ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼å¹¶é¦–å­—æ¯å¤§å†™ï¼‰
                display_name = human_name.replace('_', ' ').title()

                # å°è¯•ä»åç§°ä¸­æå–æ€§åˆ«å’Œé£æ ¼
                gender = None
                style = None
                if 'male' in human_name.lower():
                    gender = 'male'
                elif 'female' in human_name.lower():
                    gender = 'female'

                if 'formal' in human_name.lower():
                    style = 'formal'
                elif 'casual' in human_name.lower():
                    style = 'casual'
                elif 'tech' in human_name.lower():
                    style = 'tech'

                human = DigitalHuman(
                    name=human_name,
                    display_name=display_name,
                    gender=gender,
                    style=style,
                    is_active=True,
                    is_default=(human_name == 'default'),
                    **videos
                )
                db.add(human)
                logger.info(f"  âœ¨ æ–°å¢è™šæ‹Ÿäººå½¢è±¡: {display_name}")

            total_count += 1

        db.commit()
        logger.info(f"âœ… æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {total_count} ä¸ªè™šæ‹Ÿäººå½¢è±¡")
        return total_count

    except Exception as e:
        logger.error(f"âŒ æ‰«æå¤±è´¥: {str(e)}")
        logger.exception(e)
        return 0
    finally:
        db.close()


@router.get("/interviewers")
async def get_interviewers(db: Session = Depends(get_digital_interviewer_db)):
    """è·å–æ‰€æœ‰é¢è¯•å®˜åˆ—è¡¨"""
    interviewers = db.query(InterviewerProfile).all()
    return {"interviewers": [interviewer.to_dict() for interviewer in interviewers]}


@router.get("/digital-humans")
async def get_digital_humans(db: Session = Depends(get_digital_interviewer_db)):
    """è·å–æ‰€æœ‰å¯ç”¨çš„è™šæ‹Ÿäººå½¢è±¡"""
    from backend.models.db_models import DigitalHuman

    humans = db.query(DigitalHuman).filter_by(is_active=True).all()

    return {
        "digital_humans": [human.to_dict() for human in humans]
    }


@router.post("/interviewers/upload")
async def upload_interviewer_profile(
    file: UploadFile = File(...),
    db: Session = Depends(get_digital_interviewer_db)
):
    """ä¸Šä¼ é¢è¯•å®˜ç”»åƒæ–‡ä»¶"""
    from loguru import logger

    try:
        logger.info(f"ğŸ“¤ å¼€å§‹ä¸Šä¼ é¢è¯•å®˜ç”»åƒ: {file.filename}")

        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        content = await file.read()
        file_hash = hashlib.md5(content).hexdigest()
        logger.info(f"ğŸ“ æ–‡ä»¶å“ˆå¸Œ: {file_hash}")

        # æ£€æŸ¥æ˜¯å¦å·²å¯¼å…¥
        existing = db.query(InterviewerProfileRegistry).filter_by(file_hash=file_hash).first()
        if existing:
            logger.info(f"âš ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡å¯¼å…¥")
            return {"message": "è¯¥æ–‡ä»¶å·²å¯¼å…¥", "interviewer_id": existing.interviewer_profile_id}

        # ä¿å­˜æ–‡ä»¶
        file_path = os.path.join(PROFILE_DIR, file.filename)
        with open(file_path, 'wb') as f:
            f.write(content)
        logger.info(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")

        # è§£æç”»åƒ
        logger.info(f"ğŸ” å¼€å§‹è§£æç”»åƒæ–‡ä»¶...")
        parser = InterviewerProfileParser()
        profile_data = await parser.parse_file(file_path)
        logger.info(f"âœ… è§£æå®Œæˆï¼Œæå–åˆ°çš„æ•°æ®: {profile_data.keys()}")
        logger.info(f"ğŸ“‹ å§“å: {profile_data.get('name', 'N/A')}")
        logger.info(f"ğŸ“‹ ä¸“ä¸šé¢†åŸŸ: {profile_data.get('expertise_areas', 'N/A')}")

        # ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
        system_prompt = await parser.generate_system_prompt(profile_data)
        profile_data['system_prompt'] = system_prompt
        logger.info(f"ğŸ’¬ ç³»ç»Ÿæç¤ºè¯å·²ç”Ÿæˆ (é•¿åº¦: {len(system_prompt)})")

        # ä¿å­˜åˆ°æ•°æ®åº“
        interviewer = InterviewerProfile(**profile_data)
        db.add(interviewer)
        db.commit()
        db.refresh(interviewer)
        logger.info(f"âœ… é¢è¯•å®˜å·²ä¿å­˜åˆ°æ•°æ®åº“ï¼ŒID: {interviewer.id}")

        # æ³¨å†Œæ–‡ä»¶
        registry = InterviewerProfileRegistry(
            filename=file.filename,
            file_hash=file_hash,
            interviewer_profile_id=interviewer.id,
            interviewer_name=interviewer.name
        )
        db.add(registry)
        db.commit()
        logger.info(f"âœ… æ–‡ä»¶å·²æ³¨å†Œ")

        return {"message": "ä¸Šä¼ æˆåŠŸ", "interviewer": interviewer.to_dict()}

    except Exception as e:
        logger.error(f"âŒ ä¸Šä¼ å¤±è´¥: {str(e)}")
        logger.exception(e)
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/interviewers/{interviewer_id}")
async def delete_interviewer(
    interviewer_id: int,
    db: Session = Depends(get_digital_interviewer_db)
):
    """åˆ é™¤é¢è¯•å®˜"""
    try:
        interviewer = db.query(InterviewerProfile).filter_by(id=interviewer_id).first()
        if not interviewer:
            raise HTTPException(status_code=404, detail="é¢è¯•å®˜ä¸å­˜åœ¨")

        db.delete(interviewer)
        db.commit()

        return {"message": "åˆ é™¤æˆåŠŸ"}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/sessions/start")
async def start_interview_session(
    interviewer_id: int = Form(...),
    interview_type: str = Form(...),
    candidate_name: str = Form(None),
    digital_human_id: int = Form(None),
    experience_set_ids: str = Form(None),  # JSONæ•°ç»„å­—ç¬¦ä¸²ï¼Œå¦‚ "[1,2,3]"
    experience_mode: str = Form("none"),   # none/reference/strict/mixed
    max_rounds: int = Form(5),             # æœ€å¤§é¢è¯•è½®æ•°ï¼Œé»˜è®¤5è½®
    db: Session = Depends(get_digital_interviewer_db)
):
    """å¼€å§‹é¢è¯•ä¼šè¯"""
    from backend.models.db_models import DigitalHuman
    import json as json_module

    try:
        interviewer = db.query(InterviewerProfile).filter_by(id=interviewer_id).first()
        if not interviewer:
            raise HTTPException(status_code=404, detail="é¢è¯•å®˜ä¸å­˜åœ¨")

        # éªŒè¯è™šæ‹Ÿäººå½¢è±¡
        digital_human = None
        if digital_human_id:
            digital_human = db.query(DigitalHuman).filter_by(id=digital_human_id).first()
            if not digital_human:
                raise HTTPException(status_code=404, detail="è™šæ‹Ÿäººå½¢è±¡ä¸å­˜åœ¨")

        # è§£æé¢ç»é›†IDåˆ—è¡¨
        parsed_set_ids = []
        if experience_set_ids and experience_set_ids.strip():
            try:
                parsed_set_ids = json_module.loads(experience_set_ids)
                if not isinstance(parsed_set_ids, list):
                    parsed_set_ids = []
            except:
                parsed_set_ids = []

        # åˆ›å»ºä¼šè¯
        session_id = str(uuid.uuid4())
        session = InterviewSession(
            session_id=session_id,
            interviewer_profile_id=interviewer_id,
            digital_human_id=digital_human_id,
            interviewer_name=interviewer.name,
            interviewer_title=interviewer.title,
            interview_type=interview_type,
            candidate_name=candidate_name,
            max_rounds=max_rounds,
            status="in_progress"
        )

        db.add(session)
        db.commit()
        db.refresh(session)

        # è¿”å›ä¼šè¯ä¿¡æ¯å’Œè™šæ‹Ÿäººå½¢è±¡è§†é¢‘è·¯å¾„
        response_data = {
            "message": "ä¼šè¯åˆ›å»ºæˆåŠŸ",
            "session_id": session.session_id,
            "session": session.to_dict(),
            "experience_set_ids": parsed_set_ids,
            "experience_mode": experience_mode,
            "max_rounds": max_rounds
        }

        if digital_human:
            # æ„å»ºå®Œæ•´çš„è§†é¢‘URLï¼ˆåŒ…å«åç«¯æœåŠ¡å™¨åœ°å€ï¼‰
            base_url = "http://localhost:8001"  # åç«¯æœåŠ¡å™¨åœ°å€ï¼ˆæ³¨æ„æ˜¯8001ç«¯å£ï¼‰
            response_data["digital_human_videos"] = {
                "idle": f"{base_url}{digital_human.video_idle}",
                "speaking": f"{base_url}{digital_human.video_speaking}",
                "listening": f"{base_url}{digital_human.video_listening}",
                "thinking": f"{base_url}{digital_human.video_thinking}"
            }

        return response_data

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/sessions/{session_id}")
async def get_interview_session(
    session_id: str,
    db: Session = Depends(get_digital_interviewer_db)
):
    """è·å–é¢è¯•ä¼šè¯è¯¦æƒ…"""
    session = db.query(InterviewSession).filter_by(session_id=session_id).first()
    if not session:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")

    return {"session": session.to_dict()}


@router.get("/sessions/{session_id}/rounds")
async def get_interview_rounds(
    session_id: str,
    db: Session = Depends(get_digital_interviewer_db)
):
    """è·å–é¢è¯•è½®æ¬¡æ•°æ®"""
    from backend.models.db_models import InterviewRound

    session = db.query(InterviewSession).filter_by(session_id=session_id).first()
    if not session:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")

    rounds = db.query(InterviewRound).filter_by(session_id=session.id).order_by(InterviewRound.round_number).all()
    return {"rounds": [r.to_dict() for r in rounds]}


@router.get("/sessions/{session_id}/evaluation")
async def get_interview_evaluation(
    session_id: str,
    db: Session = Depends(get_digital_interviewer_db)
):
    """è·å–é¢è¯•è¯„ä¼°ç»“æœ"""
    from backend.models.db_models import InterviewEvaluation, InterviewRound

    session = db.query(InterviewSession).filter_by(session_id=session_id).first()
    if not session:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")

    # å°è¯•è·å–å·²ä¿å­˜çš„è¯„ä¼°
    evaluation = db.query(InterviewEvaluation).filter_by(session_id=session.id).first()

    if evaluation:
        return {"evaluation": evaluation.to_dict()}

    # å¦‚æœæ²¡æœ‰ä¿å­˜çš„è¯„ä¼°ï¼Œä»è½®æ¬¡æ•°æ®ä¸­æ±‡æ€»
    rounds = db.query(InterviewRound).filter_by(session_id=session.id).all()

    if not rounds:
        # æ²¡æœ‰ä»»ä½•è½®æ¬¡æ•°æ®ï¼Œè¿”å›ç©ºè¯„ä¼°
        return {
            "evaluation": {
                "technical_score": 0,
                "communication_score": 0,
                "problem_solving_score": 0,
                "cultural_fit_score": 0,
                "total_score": 0,
                "performance_level": "æœªå®Œæˆ",
                "strengths": [],
                "weaknesses": [],
                "suggestions": ["é¢è¯•æœªå®Œæˆï¼Œæ— æ³•ç”Ÿæˆè¯„ä¼°"]
            }
        }

    # ä»è½®æ¬¡è¯„ä¼°æ•°æ®ä¸­æ±‡æ€»
    total_technical = 0
    total_communication = 0
    total_problem_solving = 0
    total_cultural_fit = 0
    count = 0

    for r in rounds:
        if r.evaluation_data:
            total_technical += r.evaluation_data.get("technical_score", 0)
            total_communication += r.evaluation_data.get("communication_score", 0)
            total_problem_solving += r.evaluation_data.get("problem_solving_score", 0)
            total_cultural_fit += r.evaluation_data.get("cultural_fit_score", 0)
            count += 1

    if count > 0:
        avg_technical = round(total_technical / count, 1)
        avg_communication = round(total_communication / count, 1)
        avg_problem_solving = round(total_problem_solving / count, 1)
        avg_cultural_fit = round(total_cultural_fit / count, 1)
        total_score = round(avg_technical + avg_communication + avg_problem_solving + avg_cultural_fit, 1)

        # ç¡®å®šè¡¨ç°ç­‰çº§
        if total_score >= 32:
            level = "ä¼˜ç§€"
        elif total_score >= 24:
            level = "è‰¯å¥½"
        elif total_score >= 16:
            level = "åˆæ ¼"
        else:
            level = "å¾…æå‡"
    else:
        avg_technical = avg_communication = avg_problem_solving = avg_cultural_fit = 0
        total_score = 0
        level = "æœªè¯„ä¼°"

    return {
        "evaluation": {
            "technical_score": avg_technical,
            "communication_score": avg_communication,
            "problem_solving_score": avg_problem_solving,
            "cultural_fit_score": avg_cultural_fit,
            "total_score": total_score,
            "performance_level": level,
            "strengths": [],
            "weaknesses": [],
            "suggestions": []
        }
    }


@router.get("/sessions/{session_id}/download-pdf")
async def download_pdf_report(
    session_id: str,
    db: Session = Depends(get_digital_interviewer_db)
):
    """ä¸‹è½½é¢è¯•æŠ¥å‘ŠPDF"""
    from backend.models.db_models import InterviewRound, InterviewEvaluation
    from fastapi.responses import Response
    from datetime import datetime

    # è·å–ä¼šè¯
    session = db.query(InterviewSession).filter_by(session_id=session_id).first()
    if not session:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")

    # è·å–è½®æ¬¡æ•°æ®
    rounds = db.query(InterviewRound).filter_by(session_id=session.id).order_by(InterviewRound.round_number).all()

    # è·å–è¯„ä¼°æ•°æ®
    evaluation = db.query(InterviewEvaluation).filter_by(session_id=session.id).first()

    # å¦‚æœæ²¡æœ‰è¯„ä¼°ï¼Œä»è½®æ¬¡æ±‡æ€»
    if not evaluation:
        total_technical = total_communication = total_problem_solving = total_cultural_fit = 0
        count = 0
        for r in rounds:
            if r.evaluation_data:
                total_technical += r.evaluation_data.get("technical_score", 0)
                total_communication += r.evaluation_data.get("communication_score", 0)
                total_problem_solving += r.evaluation_data.get("problem_solving_score", 0)
                total_cultural_fit += r.evaluation_data.get("cultural_fit_score", 0)
                count += 1

        if count > 0:
            eval_data = {
                "technical_score": round(total_technical / count, 1),
                "communication_score": round(total_communication / count, 1),
                "problem_solving_score": round(total_problem_solving / count, 1),
                "cultural_fit_score": round(total_cultural_fit / count, 1),
            }
            eval_data["total_score"] = round(sum(eval_data.values()), 1)
        else:
            eval_data = {
                "technical_score": 0,
                "communication_score": 0,
                "problem_solving_score": 0,
                "cultural_fit_score": 0,
                "total_score": 0,
            }
    else:
        eval_data = {
            "technical_score": evaluation.technical_score or 0,
            "communication_score": evaluation.communication_score or 0,
            "problem_solving_score": evaluation.problem_solving_score or 0,
            "cultural_fit_score": evaluation.cultural_fit_score or 0,
            "total_score": evaluation.total_score or 0,
        }

    # ç”ŸæˆHTMLå†…å®¹
    rounds_html = ""
    for i, r in enumerate(rounds):
        eval_info = ""
        if r.evaluation_data:
            eval_info = f"""
            <p><strong>è¯„åˆ†:</strong> æŠ€æœ¯ {r.evaluation_data.get('technical_score', 0)}/10 |
            æ²Ÿé€š {r.evaluation_data.get('communication_score', 0)}/10 |
            é—®é¢˜è§£å†³ {r.evaluation_data.get('problem_solving_score', 0)}/10</p>
            """
        rounds_html += f"""
        <div class="round">
            <h3>ç¬¬ {i + 1} è½®</h3>
            <p><strong>é—®é¢˜:</strong> {r.question}</p>
            <p><strong>å›ç­”:</strong> {r.answer or 'æœªå›ç­”'}</p>
            {eval_info}
        </div>
        """

    interview_type_map = {
        "technical": "æŠ€æœ¯é¢è¯•",
        "hr": "HRé¢è¯•",
        "behavioral": "è¡Œä¸ºé¢è¯•"
    }

    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <title>é¢è¯•æŠ¥å‘Š</title>
        <style>
            @page {{ size: A4; margin: 2cm; }}
            body {{ font-family: 'SimSun', 'Microsoft YaHei', sans-serif; line-height: 1.6; color: #333; }}
            h1 {{ color: #1976d2; border-bottom: 2px solid #1976d2; padding-bottom: 10px; }}
            h2 {{ color: #333; margin-top: 30px; }}
            h3 {{ color: #1976d2; }}
            .info-grid {{ display: grid; grid-template-columns: 1fr 1fr; gap: 10px; margin: 20px 0; }}
            .info-item {{ padding: 10px; background: #f5f5f5; border-radius: 5px; }}
            .scores {{ display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px; margin: 20px 0; }}
            .score-item {{ text-align: center; padding: 15px; background: #e3f2fd; border-radius: 8px; }}
            .score-value {{ font-size: 24px; font-weight: bold; color: #1976d2; }}
            .total-score {{ text-align: center; padding: 20px; background: #1976d2; color: white; border-radius: 10px; margin: 20px 0; }}
            .total-score .value {{ font-size: 48px; font-weight: bold; }}
            .round {{ margin: 20px 0; padding: 15px; background: #fafafa; border-left: 4px solid #1976d2; border-radius: 0 8px 8px 0; }}
            .footer {{ margin-top: 40px; text-align: center; color: #666; font-size: 12px; }}
        </style>
    </head>
    <body>
        <h1>é¢è¯•æŠ¥å‘Š</h1>

        <h2>åŸºæœ¬ä¿¡æ¯</h2>
        <div class="info-grid">
            <div class="info-item"><strong>å€™é€‰äºº:</strong> {session.candidate_name or 'æœªæä¾›'}</div>
            <div class="info-item"><strong>é¢è¯•å®˜:</strong> {session.interviewer_name}</div>
            <div class="info-item"><strong>é¢è¯•ç±»å‹:</strong> {interview_type_map.get(session.interview_type, session.interview_type)}</div>
            <div class="info-item"><strong>é¢è¯•æ—¶é—´:</strong> {session.created_at.strftime('%Y-%m-%d %H:%M')}</div>
        </div>

        <h2>ç»¼åˆè¯„åˆ†</h2>
        <div class="total-score">
            <div class="value">{eval_data['total_score']}/40</div>
            <div>ç»¼åˆå¾—åˆ†</div>
        </div>
        <div class="scores">
            <div class="score-item">
                <div class="score-value">{eval_data['technical_score']}</div>
                <div>æŠ€æœ¯èƒ½åŠ›</div>
            </div>
            <div class="score-item">
                <div class="score-value">{eval_data['communication_score']}</div>
                <div>æ²Ÿé€šèƒ½åŠ›</div>
            </div>
            <div class="score-item">
                <div class="score-value">{eval_data['problem_solving_score']}</div>
                <div>é—®é¢˜è§£å†³</div>
            </div>
            <div class="score-item">
                <div class="score-value">{eval_data['cultural_fit_score']}</div>
                <div>æ–‡åŒ–åŒ¹é…</div>
            </div>
        </div>

        <h2>é¢è¯•è¯¦æƒ…</h2>
        {rounds_html if rounds_html else '<p>æš‚æ— é¢è¯•è½®æ¬¡æ•°æ®</p>'}

        <div class="footer">
            <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p>VividCrowd AI æ•°å­—é¢è¯•å®˜ç³»ç»Ÿ</p>
        </div>
    </body>
    </html>
    """

    # å°è¯•ä½¿ç”¨ WeasyPrint ç”Ÿæˆ PDF
    try:
        from weasyprint import HTML
        pdf_bytes = HTML(string=html_content).write_pdf()
        return Response(
            content=pdf_bytes,
            media_type="application/pdf",
            headers={
                "Content-Disposition": f"attachment; filename=interview_report_{session_id}.pdf"
            }
        )
    except ImportError:
        # å¦‚æœæ²¡æœ‰å®‰è£… WeasyPrintï¼Œè¿”å› HTML
        return Response(
            content=html_content.encode('utf-8'),
            media_type="text/html",
            headers={
                "Content-Disposition": f"attachment; filename=interview_report_{session_id}.html"
            }
        )


@router.websocket("/training/ws/{session_id}")
async def interview_websocket(websocket: WebSocket, session_id: str):
    """é¢è¯•WebSocketç«¯ç‚¹ - å®Œæ•´å®ç°"""
    await websocket.accept()
    db = None
    orchestrator = None
    interview_active = False

    try:
        # 1. è·å–æ•°æ®åº“ä¼šè¯
        db = next(get_digital_interviewer_db())

        # 2. åˆå§‹åŒ–ç¼–æ’å™¨ï¼ˆä¼ é€’API Keyï¼‰
        from backend.apps.digital_interviewer.services.interviewer_orchestrator import InterviewOrchestrator
        from backend.core.config import settings
        orchestrator = InterviewOrchestrator(
            db=db,
            api_key=settings.DASHSCOPE_API_KEY,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )

        # 3. ç­‰å¾…å®¢æˆ·ç«¯å‘é€å¼€å§‹ä¿¡å·
        await websocket.send_json({
            "type": "ready",
            "message": "è¿æ¥æˆåŠŸï¼Œç­‰å¾…å¼€å§‹é¢è¯•"
        })

        # 4. é¢è¯•å¾ªç¯
        while True:
            # æ¥æ”¶æ¶ˆæ¯
            data = await websocket.receive_json()

            if data.get("type") == "start_interview":
                # å¼€å§‹é¢è¯•å¹¶æ¨é€ç¬¬ä¸€ä¸ªé—®é¢˜
                if not interview_active:
                    try:
                        # è·å–é¢ç»é…ç½®ï¼ˆä»å®¢æˆ·ç«¯ä¼ é€’æˆ–ä»ä¼šè¯ä¸­è·å–ï¼‰
                        experience_set_ids = data.get("experience_set_ids", [])
                        experience_mode = data.get("experience_mode", "none")

                        first_question = await orchestrator.start_interview(
                            session_id,
                            experience_set_ids=experience_set_ids,
                            experience_mode=experience_mode
                        )
                        interview_active = True
                        await websocket.send_json({
                            "type": "question",
                            "content": first_question["question"],
                            "round_number": first_question["round_number"],
                            "video_state": "speaking"
                        })
                    except Exception as e:
                        await websocket.send_json({
                            "type": "error",
                            "message": f"å¼€å§‹é¢è¯•å¤±è´¥: {str(e)}"
                        })

            elif data.get("type") == "end_interview":
                # ç»“æŸé¢è¯•
                if interview_active:
                    try:
                        final_eval = await orchestrator.end_interview(session_id)
                        await websocket.send_json({
                            "type": "interview_end",
                            "evaluation": final_eval,
                            "message": "é¢è¯•å·²ç»“æŸ"
                        })
                        interview_active = False
                        break
                    except Exception as e:
                        await websocket.send_json({
                            "type": "error",
                            "message": f"ç»“æŸé¢è¯•å¤±è´¥: {str(e)}"
                        })

            elif data.get("type") == "answer" and interview_active:
                # æ¨é€listeningçŠ¶æ€
                await websocket.send_json({
                    "type": "video_state",
                    "state": "listening"
                })

                # å¤„ç†å›ç­”
                try:
                    result = await orchestrator.process_answer(
                        session_id=session_id,
                        answer=data.get("content", "")
                    )

                    # æ¨é€thinkingçŠ¶æ€
                    await websocket.send_json({
                        "type": "video_state",
                        "state": "thinking"
                    })

                    # æ¨é€è¯„ä¼°ç»“æœ
                    await websocket.send_json({
                        "type": "evaluation",
                        "data": result.get("evaluation", {})
                    })

                    # æ£€æŸ¥æ˜¯å¦ç»§ç»­
                    if result.get("action") == "continue":
                        # æ¨é€ä¸‹ä¸€ä¸ªé—®é¢˜
                        await websocket.send_json({
                            "type": "question",
                            "content": result["question"],
                            "round_number": result["round_number"],
                            "video_state": "speaking"
                        })
                    else:
                        # é¢è¯•ç»“æŸ
                        await websocket.send_json({
                            "type": "interview_end",
                            "evaluation": result.get("final_evaluation", {}),
                            "message": "é¢è¯•å·²ç»“æŸ"
                        })
                        break

                except Exception as e:
                    await websocket.send_json({
                        "type": "error",
                        "message": f"å¤„ç†å›ç­”å¤±è´¥: {str(e)}"
                    })

    except WebSocketDisconnect:
        # WebSocketæ–­å¼€æ—¶ï¼Œæ›´æ–°ä¼šè¯çŠ¶æ€ä¸ºcompleted
        if db:
            try:
                session = db.query(InterviewSession).filter_by(session_id=session_id).first()
                if session and session.status == "in_progress":
                    from datetime import datetime
                    session.status = "completed"
                    session.completed_at = datetime.utcnow()
                    if session.started_at:
                        session.duration_seconds = int((session.completed_at - session.started_at).total_seconds())
                    db.commit()
            except Exception as e:
                print(f"æ›´æ–°ä¼šè¯çŠ¶æ€å¤±è´¥: {e}")
    except Exception as e:
        try:
            await websocket.send_json({
                "type": "error",
                "message": f"WebSocketé”™è¯¯: {str(e)}"
            })
        except:
            pass
    finally:
        # ç¡®ä¿ä¼šè¯çŠ¶æ€è¢«æ›´æ–°
        if db:
            try:
                session = db.query(InterviewSession).filter_by(session_id=session_id).first()
                if session and session.status == "in_progress":
                    from datetime import datetime
                    session.status = "completed"
                    session.completed_at = datetime.utcnow()
                    if session.started_at:
                        session.duration_seconds = int((session.completed_at - session.started_at).total_seconds())
                    db.commit()
            except:
                pass
            db.close()


# ==================== è¯­éŸ³æœåŠ¡ç«¯ç‚¹ ====================

@router.post("/audio/transcribe")
async def transcribe_audio_endpoint(file: UploadFile = File(...)):
    """éŸ³é¢‘è½¬æ–‡å­— (ASR) - ç”¨äºè¯†åˆ«å€™é€‰äººçš„è¯­éŸ³å›ç­”"""
    from backend.apps.digital_customer.services.audio_service import transcribe_audio
    from loguru import logger

    try:
        content = await file.read()
        extension = file.filename.split(".")[-1] if "." in file.filename else "wav"
        text = await transcribe_audio(content, extension)
        return {"text": text}
    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/audio/synthesize")
async def synthesize_audio_endpoint(
    text: str = Form(...),
    voice: str = Form("longxiaochun")
):
    """æ–‡å­—è½¬éŸ³é¢‘ (TTS) - ç”¨äºé¢è¯•å®˜è¯­éŸ³æé—®"""
    from backend.apps.digital_customer.services.audio_service import synthesize_audio
    from fastapi.responses import Response
    from loguru import logger

    try:
        audio_data = await synthesize_audio(text, voice)
        return Response(content=audio_data, media_type="audio/mpeg")
    except Exception as e:
        logger.error(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/sessions")
async def get_interview_sessions(db: Session = Depends(get_digital_interviewer_db)):
    """è·å–æ‰€æœ‰é¢è¯•ä¼šè¯åˆ—è¡¨"""
    sessions = db.query(InterviewSession).order_by(InterviewSession.created_at.desc()).all()
    return {"sessions": [session.to_dict() for session in sessions]}


# ==================== é¢ç»ç®¡ç†ç«¯ç‚¹ ====================

@router.post("/experience-sets/upload-stream")
async def upload_experience_set_stream(
    name: str = Form(...),
    interview_type: str = Form("technical"),
    description: str = Form(None),
    company: str = Form(None),
    position: str = Form(None),
    file: UploadFile = File(...),
    db: Session = Depends(get_digital_interviewer_db)
):
    """ä¸Šä¼ PDFé¢ç»å¹¶åˆ›å»ºé¢ç»é›†ï¼ˆSSEæµå¼è¿”å›è¿›åº¦ï¼‰"""
    from fastapi.responses import StreamingResponse
    from backend.core.config import settings
    from backend.apps.digital_interviewer.services.experience_parser import ExperienceParser
    from loguru import logger
    import json as json_module

    # å…ˆè¯»å–æ–‡ä»¶å†…å®¹ï¼ˆåœ¨ç”Ÿæˆå™¨å¤–éƒ¨è¯»å–ï¼Œé¿å…æ–‡ä»¶å…³é—­é—®é¢˜ï¼‰
    file_content = await file.read()
    filename = file.filename

    async def generate_progress():
        try:
            logger.info(f"å¼€å§‹ä¸Šä¼ é¢ç»: {filename}")

            # éªŒè¯æ–‡ä»¶ç±»å‹
            if not filename.lower().endswith('.pdf'):
                yield f"data: {json_module.dumps({'type': 'error', 'message': 'åªæ”¯æŒPDFæ–‡ä»¶'})}\n\n"
                return

            # ä¿å­˜æ–‡ä»¶
            file_path = os.path.join(EXPERIENCE_DIR, filename)
            with open(file_path, 'wb') as f:
                f.write(file_content)

            yield f"data: {json_module.dumps({'type': 'progress', 'current': 0, 'total': 100, 'message': 'æ–‡ä»¶å·²ä¿å­˜ï¼Œå¼€å§‹è§£æ...'})}\n\n"

            # åˆ›å»ºè§£æå™¨
            parser = ExperienceParser(
                api_key=settings.DASHSCOPE_API_KEY,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )

            # æµå¼è§£æ
            parse_result = None
            async for progress in parser.parse_pdf_stream(file_path):
                if progress["type"] == "complete":
                    parse_result = progress["result"]
                yield f"data: {json_module.dumps(progress)}\n\n"

            if not parse_result:
                yield f"data: {json_module.dumps({'type': 'error', 'message': 'è§£æå¤±è´¥'})}\n\n"
                return

            # ä¿å­˜åˆ°æ•°æ®åº“
            yield f"data: {json_module.dumps({'type': 'progress', 'current': 100, 'total': 100, 'message': 'æ­£åœ¨ä¿å­˜åˆ°æ•°æ®åº“...'})}\n\n"

            experience_set = InterviewExperienceSet(
                name=name,
                description=description or parse_result.get("title"),
                source_filename=filename,
                company=company or parse_result.get("company"),
                position=position or parse_result.get("position"),
                interview_type=interview_type or parse_result.get("interview_type", "technical"),
                question_count=len(parse_result.get("questions", [])),
                is_active=True
            )
            db.add(experience_set)
            db.flush()

            # ä¿å­˜é—®é¢˜
            for q in parse_result.get("questions", []):
                knowledge = InterviewKnowledge(
                    interview_type=interview_type,
                    category=q.get("category"),
                    content=q.get("question", ""),
                    difficulty_level=q.get("difficulty"),
                    source_filename=filename,
                    experience_set_id=experience_set.id,
                    question_text=q.get("question", ""),
                    reference_answer=q.get("answer"),
                    tags=json_module.dumps(q.get("tags")) if q.get("tags") else None
                )
                db.add(knowledge)

            db.commit()
            db.refresh(experience_set)

            yield f"data: {json_module.dumps({'type': 'done', 'message': 'ä¸Šä¼ æˆåŠŸ', 'experience_set': experience_set.to_dict()})}\n\n"

        except Exception as e:
            logger.error(f"ä¸Šä¼ é¢ç»å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            yield f"data: {json_module.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        generate_progress(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@router.post("/experience-sets/upload")
async def upload_experience_set(
    name: str = Form(...),
    interview_type: str = Form("technical"),
    description: str = Form(None),
    company: str = Form(None),
    position: str = Form(None),
    file: UploadFile = File(...),
    db: Session = Depends(get_digital_interviewer_db)
):
    """ä¸Šä¼ PDFé¢ç»å¹¶åˆ›å»ºé¢ç»é›†"""
    from loguru import logger
    from backend.core.config import settings

    try:
        logger.info(f"å¼€å§‹ä¸Šä¼ é¢ç»: {file.filename}")

        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="åªæ”¯æŒPDFæ–‡ä»¶")

        # ä¿å­˜æ–‡ä»¶
        content = await file.read()
        file_path = os.path.join(EXPERIENCE_DIR, file.filename)
        with open(file_path, 'wb') as f:
            f.write(content)
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")

        # åˆ›å»ºé¢ç»é›†
        service = ExperienceService(
            db=db,
            api_key=settings.DASHSCOPE_API_KEY,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )

        experience_set = await service.create_experience_set(
            name=name,
            file_path=file_path,
            interview_type=interview_type,
            description=description,
            company=company,
            position=position
        )

        return {
            "message": "ä¸Šä¼ æˆåŠŸ",
            "experience_set": experience_set.to_dict()
        }

    except Exception as e:
        logger.error(f"ä¸Šä¼ é¢ç»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/experience-sets")
async def list_experience_sets(
    interview_type: str = None,
    db: Session = Depends(get_digital_interviewer_db)
):
    """è·å–é¢ç»é›†åˆ—è¡¨"""
    service = ExperienceService(db=db)
    sets = service.get_experience_sets(interview_type=interview_type)
    return {"experience_sets": [s.to_dict() for s in sets]}


@router.get("/experience-sets/{set_id}")
async def get_experience_set(
    set_id: int,
    db: Session = Depends(get_digital_interviewer_db)
):
    """è·å–å•ä¸ªé¢ç»é›†è¯¦æƒ…"""
    service = ExperienceService(db=db)
    experience_set = service.get_experience_set(set_id)
    if not experience_set:
        raise HTTPException(status_code=404, detail="é¢ç»é›†ä¸å­˜åœ¨")
    return {"experience_set": experience_set.to_dict()}


@router.get("/experience-sets/{set_id}/questions")
async def get_experience_questions(
    set_id: int,
    db: Session = Depends(get_digital_interviewer_db)
):
    """è·å–é¢ç»é›†ä¸­çš„é—®é¢˜"""
    service = ExperienceService(db=db)
    questions = service.get_questions_by_set(set_id)
    return {"questions": [q.to_dict() for q in questions]}


@router.delete("/experience-sets/{set_id}")
async def delete_experience_set(
    set_id: int,
    db: Session = Depends(get_digital_interviewer_db)
):
    """åˆ é™¤é¢ç»é›†"""
    service = ExperienceService(db=db)
    success = service.delete_experience_set(set_id)
    if not success:
        raise HTTPException(status_code=404, detail="é¢ç»é›†ä¸å­˜åœ¨")
    return {"message": "åˆ é™¤æˆåŠŸ"}


# ==================== ç®€å†ç®¡ç†API ====================

@router.post("/resumes/upload")
async def upload_resume(
    file: UploadFile = File(...),
    candidate_name: str = Form(None),
    candidate_id: str = Form(None),
    db: Session = Depends(get_digital_interviewer_db)
):
    """ä¸Šä¼ å¹¶è§£æç®€å†"""
    from loguru import logger
    from backend.models.db_models import CandidateResume, ResumeAnalysis
    from backend.apps.digital_interviewer.services.resume_parser import ResumeParser

    try:
        logger.info(f"ğŸ“¤ å¼€å§‹ä¸Šä¼ ç®€å†: {file.filename}")

        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        file_ext = file.filename.split('.')[-1].lower()
        if file_ext not in ['pdf', 'docx', 'doc', 'jpg', 'jpeg', 'png']:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")

        # ä¿å­˜æ–‡ä»¶
        file_path = os.path.join(RESUME_DIR, f"{uuid.uuid4()}_{file.filename}")
        content = await file.read()
        with open(file_path, 'wb') as f:
            f.write(content)

        logger.info(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜: {file_path}")

        # è§£æç®€å†
        parser = ResumeParser()
        parse_result = parser.parse_resume(file_path)

        if not parse_result['success']:
            raise HTTPException(status_code=400, detail=parse_result['error'])

        # åˆ›å»ºç®€å†è®°å½•
        resume = CandidateResume(
            candidate_id=candidate_id,
            candidate_name=candidate_name,
            file_path=file_path,
            file_type=file_ext,
            file_hash=parse_result['file_hash'],
            parse_status='completed',
            parsed_data=parse_result['structured_data']
        )

        # è¯„ä¼°ç®€å†è´¨é‡
        quality_result = parser.evaluate_quality(
            parse_result['structured_data'],
            parse_result['raw_text']
        )
        resume.quality_score = quality_result['quality_score']
        resume.completeness_score = quality_result['completeness_score']
        resume.professionalism_score = quality_result['professionalism_score']

        db.add(resume)
        db.flush()

        # åˆ›å»ºåˆ†æè®°å½•
        tags = parser.extract_tags(parse_result['structured_data'])
        work_years = parser.calculate_work_years(parse_result['structured_data'])

        analysis = ResumeAnalysis(
            resume_id=resume.id,
            contact_info=parse_result['structured_data'].get('contact'),
            education=parse_result['structured_data'].get('education'),
            work_experience=parse_result['structured_data'].get('work_experience'),
            project_experience=parse_result['structured_data'].get('projects'),
            skills=parse_result['structured_data'].get('skills'),
            certifications=parse_result['structured_data'].get('certifications'),
            total_work_years=work_years,
            skill_tags=tags['skill_tags'],
            industry_tags=tags['industry_tags'],
            position_tags=tags['position_tags'],
            quality_issues=quality_result['issues'],
            improvement_suggestions=quality_result['suggestions'],
            risk_flags=quality_result['risk_flags']
        )

        db.add(analysis)
        db.commit()
        db.refresh(resume)

        logger.info(f"âœ… ç®€å†ä¸Šä¼ æˆåŠŸ: ID={resume.id}")

        return {
            "message": "ç®€å†ä¸Šä¼ æˆåŠŸ",
            "resume_id": resume.id,
            "resume": resume.to_dict(),
            "quality_score": resume.quality_score
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸Šä¼ ç®€å†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/resumes")
async def list_resumes(
    skip: int = 0,
    limit: int = 20,
    db: Session = Depends(get_digital_interviewer_db)
):
    """è·å–ç®€å†åˆ—è¡¨"""
    from backend.models.db_models import CandidateResume

    resumes = db.query(CandidateResume).offset(skip).limit(limit).all()
    total = db.query(CandidateResume).count()

    return {
        "resumes": [resume.to_dict() for resume in resumes],
        "total": total,
        "skip": skip,
        "limit": limit
    }


@router.get("/resumes/{resume_id}")
async def get_resume(
    resume_id: int,
    db: Session = Depends(get_digital_interviewer_db)
):
    """è·å–ç®€å†è¯¦æƒ…"""
    from backend.models.db_models import CandidateResume, ResumeAnalysis

    resume = db.query(CandidateResume).filter(CandidateResume.id == resume_id).first()
    if not resume:
        raise HTTPException(status_code=404, detail="ç®€å†ä¸å­˜åœ¨")

    analysis = db.query(ResumeAnalysis).filter(ResumeAnalysis.resume_id == resume_id).first()

    return {
        "resume": resume.to_dict(),
        "analysis": analysis.to_dict() if analysis else None
    }


@router.delete("/resumes/{resume_id}")
async def delete_resume(
    resume_id: int,
    db: Session = Depends(get_digital_interviewer_db)
):
    """åˆ é™¤ç®€å†"""
    from backend.models.db_models import CandidateResume

    resume = db.query(CandidateResume).filter(CandidateResume.id == resume_id).first()
    if not resume:
        raise HTTPException(status_code=404, detail="ç®€å†ä¸å­˜åœ¨")

    # åˆ é™¤æ–‡ä»¶
    if os.path.exists(resume.file_path):
        os.remove(resume.file_path)

    db.delete(resume)
    db.commit()

    return {"message": "åˆ é™¤æˆåŠŸ"}


# ==================== èŒä½ç®¡ç†API ====================

@router.post("/jobs")
async def create_job(
    title: str = Form(...),
    department: str = Form(None),
    requirements: str = Form(None),
    skills_required: str = Form(None),
    education_required: str = Form(None),
    experience_years_min: int = Form(None),
    job_type: str = Form(None),
    db: Session = Depends(get_digital_interviewer_db)
):
    """åˆ›å»ºèŒä½"""
    from backend.models.db_models import JobPosition
    import json

    try:
        skills_list = json.loads(skills_required) if skills_required else []
    except:
        skills_list = []

    job = JobPosition(
        title=title,
        department=department,
        requirements=requirements,
        skills_required=skills_list,
        education_required=education_required,
        experience_years_min=experience_years_min,
        job_type=job_type,
        is_active=True
    )

    db.add(job)
    db.commit()
    db.refresh(job)

    return {"message": "èŒä½åˆ›å»ºæˆåŠŸ", "job": job.to_dict()}


@router.get("/jobs")
async def list_jobs(
    is_active: bool = None,
    db: Session = Depends(get_digital_interviewer_db)
):
    """è·å–èŒä½åˆ—è¡¨"""
    from backend.models.db_models import JobPosition

    query = db.query(JobPosition)
    if is_active is not None:
        query = query.filter(JobPosition.is_active == is_active)

    jobs = query.all()
    return {"jobs": [job.to_dict() for job in jobs]}


# ==================== ç®€å†åŒ¹é…API ====================

@router.post("/resumes/{resume_id}/match")
async def match_resume_to_jobs(
    resume_id: int,
    job_ids: List[int] = None,
    db: Session = Depends(get_digital_interviewer_db)
):
    """å°†ç®€å†ä¸èŒä½è¿›è¡ŒåŒ¹é…"""
    from backend.apps.digital_interviewer.services.resume_matcher import ResumeMatcher

    try:
        matcher = ResumeMatcher(db)
        results = matcher.batch_match(resume_id, job_ids)

        return {
            "message": "åŒ¹é…å®Œæˆ",
            "resume_id": resume_id,
            "matches": results
        }

    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ==================== GDPRåˆè§„API ====================

@router.get("/candidates/{candidate_id}/export")
async def export_candidate_data(
    candidate_id: str,
    db: Session = Depends(get_digital_interviewer_db)
):
    """å¯¼å‡ºå€™é€‰äººçš„æ‰€æœ‰æ•°æ®ï¼ˆGDPRåˆè§„ï¼‰"""
    from backend.models.db_models import (
        CandidateResume, ResumeAnalysis, InterviewSession,
        InterviewRound, InterviewEvaluation
    )
    import json

    try:
        # æ”¶é›†æ‰€æœ‰ç›¸å…³æ•°æ®
        export_data = {
            "candidate_id": candidate_id,
            "export_time": datetime.utcnow().isoformat(),
            "resumes": [],
            "interviews": []
        }

        # ç®€å†æ•°æ®
        resumes = db.query(CandidateResume).filter(
            CandidateResume.candidate_id == candidate_id
        ).all()

        for resume in resumes:
            resume_data = resume.to_dict()
            analysis = db.query(ResumeAnalysis).filter(
                ResumeAnalysis.resume_id == resume.id
            ).first()
            if analysis:
                resume_data['analysis'] = analysis.to_dict()
            export_data['resumes'].append(resume_data)

        return JSONResponse(content=export_data)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/candidates/{candidate_id}/data")
async def delete_candidate_data(
    candidate_id: str,
    db: Session = Depends(get_digital_interviewer_db)
):
    """åˆ é™¤å€™é€‰äººçš„æ‰€æœ‰æ•°æ®ï¼ˆGDPRåˆè§„ï¼‰"""
    from backend.models.db_models import CandidateResume, InterviewSession

    try:
        # åˆ é™¤ç®€å†ï¼ˆçº§è”åˆ é™¤åˆ†ææ•°æ®ï¼‰
        resumes = db.query(CandidateResume).filter(
            CandidateResume.candidate_id == candidate_id
        ).all()

        deleted_count = 0
        for resume in resumes:
            # åˆ é™¤æ–‡ä»¶
            if os.path.exists(resume.file_path):
                os.remove(resume.file_path)
            db.delete(resume)
            deleted_count += 1

        # åˆ é™¤é¢è¯•ä¼šè¯ï¼ˆçº§è”åˆ é™¤è½®æ¬¡å’Œè¯„ä¼°æ•°æ®ï¼‰
        sessions = db.query(InterviewSession).filter(
            InterviewSession.candidate_id == candidate_id
        ).all()

        for session in sessions:
            db.delete(session)
            deleted_count += 1

        db.commit()

        return {
            "message": "å€™é€‰äººæ•°æ®å·²åˆ é™¤",
            "candidate_id": candidate_id,
            "deleted_items": deleted_count
        }

    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))


# ==================== è¯„åˆ†æ¨¡æ¿ç®¡ç†API ====================

@router.post("/scoring-templates")
async def create_scoring_template(
    name: str = Form(...),
    description: str = Form(None),
    job_type: str = Form(None),
    technical_weight: int = Form(25),
    communication_weight: int = Form(15),
    problem_solving_weight: int = Form(20),
    cultural_fit_weight: int = Form(10),
    innovation_weight: int = Form(10),
    teamwork_weight: int = Form(10),
    stress_handling_weight: int = Form(5),
    learning_ability_weight: int = Form(5),
    db: Session = Depends(get_digital_interviewer_db)
):
    """åˆ›å»ºè¯„åˆ†æ¨¡æ¿"""
    from backend.models.db_models import ScoringTemplate

    # éªŒè¯æƒé‡æ€»å’Œ
    total_weight = (technical_weight + communication_weight + problem_solving_weight +
                   cultural_fit_weight + innovation_weight + teamwork_weight +
                   stress_handling_weight + learning_ability_weight)

    if total_weight != 100:
        raise HTTPException(status_code=400, detail=f"æƒé‡æ€»å’Œå¿…é¡»ä¸º100ï¼Œå½“å‰ä¸º{total_weight}")

    template = ScoringTemplate(
        name=name,
        description=description,
        job_type=job_type,
        technical_weight=technical_weight,
        communication_weight=communication_weight,
        problem_solving_weight=problem_solving_weight,
        cultural_fit_weight=cultural_fit_weight,
        innovation_weight=innovation_weight,
        teamwork_weight=teamwork_weight,
        stress_handling_weight=stress_handling_weight,
        learning_ability_weight=learning_ability_weight
    )

    db.add(template)
    db.commit()
    db.refresh(template)

    return {"message": "è¯„åˆ†æ¨¡æ¿åˆ›å»ºæˆåŠŸ", "template": template.to_dict()}


@router.get("/scoring-templates")
async def list_scoring_templates(
    job_type: str = None,
    db: Session = Depends(get_digital_interviewer_db)
):
    """è·å–è¯„åˆ†æ¨¡æ¿åˆ—è¡¨"""
    from backend.models.db_models import ScoringTemplate

    query = db.query(ScoringTemplate).filter(ScoringTemplate.is_active == True)
    if job_type:
        query = query.filter(ScoringTemplate.job_type == job_type)

    templates = query.all()
    return {"templates": [t.to_dict() for t in templates]}


# ==================== é¢è¯•æ¨¡æ¿ç®¡ç†API ====================

@router.post("/interview-templates")
async def create_interview_template(
    name: str = Form(...),
    description: str = Form(None),
    job_type: str = Form(None),
    max_rounds: int = Form(5),
    difficulty_level: str = Form("medium"),
    experience_set_ids: str = Form(None),
    scoring_template_id: int = Form(None),
    db: Session = Depends(get_digital_interviewer_db)
):
    """åˆ›å»ºé¢è¯•æ¨¡æ¿"""
    from backend.models.db_models import InterviewTemplate
    import json

    try:
        exp_ids = json.loads(experience_set_ids) if experience_set_ids else []
    except:
        exp_ids = []

    template = InterviewTemplate(
        name=name,
        description=description,
        job_type=job_type,
        max_rounds=max_rounds,
        difficulty_level=difficulty_level,
        experience_set_ids=exp_ids,
        scoring_template_id=scoring_template_id
    )

    db.add(template)
    db.commit()
    db.refresh(template)

    return {"message": "é¢è¯•æ¨¡æ¿åˆ›å»ºæˆåŠŸ", "template": template.to_dict()}


# ==================== å€™é€‰äººç®¡ç†API ====================

@router.post("/candidates/batch-import")
async def batch_import_candidates(
    file: UploadFile = File(...),
    db: Session = Depends(get_digital_interviewer_db)
):
    """æ‰¹é‡å¯¼å…¥å€™é€‰äººï¼ˆExcel/CSVï¼‰"""
    from backend.models.db_models import Candidate
    import pandas as pd
    import uuid
    import io

    try:
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        file_ext = file.filename.split('.')[-1].lower()
        if file_ext not in ['xlsx', 'xls', 'csv']:
            raise HTTPException(status_code=400, detail="ä»…æ”¯æŒExcelæˆ–CSVæ ¼å¼")

        # è¯»å–æ–‡ä»¶
        content = await file.read()
        if file_ext == 'csv':
            df = pd.read_csv(io.BytesIO(content))
        else:
            df = pd.read_excel(io.BytesIO(content))

        # éªŒè¯å¿…éœ€åˆ—
        required_columns = ['å§“å']
        for col in required_columns:
            if col not in df.columns:
                raise HTTPException(status_code=400, detail=f"ç¼ºå°‘å¿…éœ€åˆ—: {col}")

        success_count = 0
        failed_count = 0
        errors = []

        for index, row in df.iterrows():
            try:
                candidate = Candidate(
                    candidate_id=str(uuid.uuid4()),
                    name=str(row['å§“å']),
                    email=str(row.get('é‚®ç®±', '')),
                    phone=str(row.get('ç”µè¯', '')),
                    position=str(row.get('èŒä½', '')),
                    source='batch_import'
                )
                db.add(candidate)
                success_count += 1
            except Exception as e:
                failed_count += 1
                errors.append(f"ç¬¬{index+2}è¡Œ: {str(e)}")

        db.commit()

        return {
            "message": "æ‰¹é‡å¯¼å…¥å®Œæˆ",
            "success_count": success_count,
            "failed_count": failed_count,
            "errors": errors[:10]
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/interviews/export")
async def export_interviews(
    start_date: str = None,
    end_date: str = None,
    db: Session = Depends(get_digital_interviewer_db)
):
    """å¯¼å‡ºé¢è¯•ç»“æœä¸ºExcel"""
    from backend.models.db_models import InterviewSession, InterviewEvaluation
    from openpyxl import Workbook
    from openpyxl.styles import Font, Alignment
    import io
    from datetime import datetime

    try:
        # æŸ¥è¯¢é¢è¯•ä¼šè¯
        query = db.query(InterviewSession).filter(InterviewSession.status == "completed")

        if start_date:
            query = query.filter(InterviewSession.created_at >= start_date)
        if end_date:
            query = query.filter(InterviewSession.created_at <= end_date)

        sessions = query.all()

        # åˆ›å»ºExcelå·¥ä½œç°¿
        wb = Workbook()
        ws = wb.active
        ws.title = "é¢è¯•ç»“æœ"

        # è®¾ç½®è¡¨å¤´
        headers = [
            "ä¼šè¯ID", "å€™é€‰äººå§“å", "é¢è¯•ç±»å‹", "é¢è¯•å®˜",
            "å¼€å§‹æ—¶é—´", "å®Œæˆæ—¶é—´", "æ€»è½®æ•°", "çŠ¶æ€",
            "æŠ€æœ¯èƒ½åŠ›", "æ²Ÿé€šè¡¨è¾¾", "é—®é¢˜è§£å†³", "æ–‡åŒ–åŒ¹é…",
            "åˆ›æ–°èƒ½åŠ›", "å›¢é˜Ÿåä½œ", "å‹åŠ›åº”å¯¹", "å­¦ä¹ èƒ½åŠ›", "æ€»åˆ†"
        ]

        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = Font(bold=True)
            cell.alignment = Alignment(horizontal='center')

        # å¡«å……æ•°æ®
        for row_idx, session in enumerate(sessions, 2):
            evaluation = db.query(InterviewEvaluation).filter(
                InterviewEvaluation.session_id == session.id
            ).first()

            ws.cell(row=row_idx, column=1, value=session.session_id)
            ws.cell(row=row_idx, column=2, value=session.candidate_name or "")
            ws.cell(row=row_idx, column=3, value=session.interview_type)
            ws.cell(row=row_idx, column=4, value=session.interviewer_name or "")
            ws.cell(row=row_idx, column=5, value=session.started_at.strftime("%Y-%m-%d %H:%M") if session.started_at else "")
            ws.cell(row=row_idx, column=6, value=session.completed_at.strftime("%Y-%m-%d %H:%M") if session.completed_at else "")
            ws.cell(row=row_idx, column=7, value=session.total_rounds)
            ws.cell(row=row_idx, column=8, value=session.status)

            if evaluation:
                ws.cell(row=row_idx, column=9, value=evaluation.technical_score or 0)
                ws.cell(row=row_idx, column=10, value=evaluation.communication_score or 0)
                ws.cell(row=row_idx, column=11, value=evaluation.problem_solving_score or 0)
                ws.cell(row=row_idx, column=12, value=evaluation.cultural_fit_score or 0)
                ws.cell(row=row_idx, column=13, value=evaluation.innovation_score or 0)
                ws.cell(row=row_idx, column=14, value=evaluation.teamwork_score or 0)
                ws.cell(row=row_idx, column=15, value=evaluation.stress_handling_score or 0)
                ws.cell(row=row_idx, column=16, value=evaluation.learning_ability_score or 0)
                ws.cell(row=row_idx, column=17, value=evaluation.total_score or 0)

        # ä¿å­˜åˆ°å†…å­˜
        output = io.BytesIO()
        wb.save(output)
        output.seek(0)

        # è¿”å›æ–‡ä»¶
        from fastapi.responses import StreamingResponse
        filename = f"interview_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

        return StreamingResponse(
            output,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
